# Qwen3 å¤§æ¨¡å‹ API æœåŠ¡

åŸºäº FastAPI çš„ Qwen3-4B-Instruct æ¨¡å‹æ¨ç†æœåŠ¡ã€‚

## ç³»ç»Ÿä¿¡æ¯

- **æ˜¾å¡**: NVIDIA GeForce RTX 4080
- **PyTorch**: 2.5.1
- **CUDA**: 12.4
- **æ¨¡å‹**: Qwen3-4B-Instruct-2507

## éƒ¨ç½²æ–¹å¼

### â˜¸ï¸ Kubernetes éƒ¨ç½²ï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰

**å¿«é€Ÿå¼€å§‹ï¼š**
```bash
# éƒ¨ç½²åˆ° Kubernetes
kubectl apply -f k8s/

# æˆ–ä½¿ç”¨ kustomize
kubectl apply -k k8s/
```

è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ [k8s/README.md](k8s/README.md)

### ğŸ³ Docker éƒ¨ç½²

**å¿«é€Ÿå¼€å§‹ï¼š**
```bash
# ä½¿ç”¨ docker-compose
docker-compose up -d
```

è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ [DOCKER.md](DOCKER.md)

### ğŸ’» æœ¬åœ°éƒ¨ç½²

## æœ¬åœ°éƒ¨ç½²

### å®‰è£…ä¾èµ–

```bash
# æ¿€æ´» conda ç¯å¢ƒ
conda activate pytorch-env

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## å¯åŠ¨æœåŠ¡

```bash
# æ–¹å¼1: ç›´æ¥è¿è¡Œ
python app.py

# æ–¹å¼2: ä½¿ç”¨ uvicorn
uvicorn app:app --host 0.0.0.0 --port 8000
```

æœåŠ¡å¯åŠ¨åï¼Œè®¿é—®ï¼š
- API æ–‡æ¡£: http://localhost:8000/docs
- å¥åº·æ£€æŸ¥: http://localhost:8000/health

## API ä½¿ç”¨ç¤ºä¾‹

### 1. å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8000/health
```

### 2. èŠå¤©æ¥å£ï¼ˆå…¼å®¹ OpenAI æ ¼å¼ï¼‰

```bash
curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ],
    "temperature": 0.7,
    "max_tokens": 1024
  }'
```

### 3. Python å®¢æˆ·ç«¯ç¤ºä¾‹

```python
import requests

url = "http://localhost:8000/v1/chat/completions"
data = {
    "messages": [
        {"role": "user", "content": "ç”¨ Python å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•"}
    ],
    "temperature": 0.7,
    "max_tokens": 2048
}

response = requests.post(url, json=data)
result = response.json()
print(result["response"])
```

### 4. ä½¿ç”¨ OpenAI SDKï¼ˆå…¼å®¹æ¨¡å¼ï¼‰

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="not-needed"
)

response = client.chat.completions.create(
    model="qwen3",
    messages=[
        {"role": "user", "content": "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"}
    ],
    temperature=0.7,
    max_tokens=1024
)

print(response.choices[0].message.content)
```

## API å‚æ•°è¯´æ˜

- `messages`: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º `[{"role": "user", "content": "..."}]`
- `temperature`: é‡‡æ ·æ¸©åº¦ (0.0-2.0)ï¼Œé»˜è®¤ 0.7ï¼Œå€¼è¶Šå¤§è¾“å‡ºè¶Šéšæœº
- `top_p`: æ ¸é‡‡æ ·å‚æ•° (0.0-1.0)ï¼Œé»˜è®¤ 0.8
- `max_tokens`: æœ€å¤§ç”Ÿæˆ token æ•°ï¼Œé»˜è®¤ 2048
- `stream`: æ˜¯å¦æµå¼è¾“å‡ºï¼ˆå½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒï¼‰

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ˜¾å­˜ä¼˜åŒ–**: æ¨¡å‹å·²ä½¿ç”¨ `bfloat16` ç²¾åº¦ï¼Œå¯è¿›ä¸€æ­¥ä½¿ç”¨é‡åŒ–
2. **æ‰¹å¤„ç†**: å½“å‰ç‰ˆæœ¬æ”¯æŒå•æ¬¡è¯·æ±‚ï¼Œå¯æ‰©å±•æ”¯æŒæ‰¹å¤„ç†
3. **æµå¼è¾“å‡º**: å¯æ·»åŠ æµå¼è¾“å‡ºæ”¯æŒä»¥æå‡ç”¨æˆ·ä½“éªŒ

## æ³¨æ„äº‹é¡¹

- é¦–æ¬¡å¯åŠ¨éœ€è¦åŠ è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU æ˜¾å­˜ï¼ˆå»ºè®®è‡³å°‘ 8GBï¼‰
- æ¨¡å‹æ–‡ä»¶ä½äº `./Qwen3-4B-Instruct-2507` ç›®å½•
